# -*- coding: utf-8 -*-
"""Q1_fixed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jLxbHZRyxeT72gdK_s89cAHhJIZDi4o2
"""

from datasets import load_dataset #load pubmedqa data

import pandas as pd
import numpy as np

pqa_artificial = pd.read_parquet("hf://datasets/qiaojin/PubMedQA/pqa_artificial/train-00000-of-00001.parquet")

pqa_labeled =  pd.read_parquet("hf://datasets/qiaojin/PubMedQA/pqa_labeled/train-00000-of-00001.parquet")

pqa_unlabeled = pd.read_parquet("hf://datasets/qiaojin/PubMedQA/pqa_unlabeled/train-00000-of-00001.parquet")

pqa_artificial.head()

pqa_labeled.head()

pqa_unlabeled.head()

# Step 1: Import Libraries
!pip install faiss-cpu
from sentence_transformers import SentenceTransformer
import faiss #FAISS library for similarity search and clustering
from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration #transformers library to work with pretrained models like BERT
import torch

from langchain.text_splitter import RecursiveCharacterTextSplitter #for sentense embeddings

# Initialize text splitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=128,
    length_function=len,
    separators=["\n\n", "\n", ". ", " ", ""]
)

documents = []

for idx, row in pqa_labeled.iterrows():
    # Extract context data (which is a dictionary)
    context_data = row['context']

    # Get the actual text contexts (stored in 'contexts' array)
    if 'contexts' in context_data:
        text_segments = context_data['contexts']

        # Process each text segment (some may be numpy strings)
        for segment in text_segments:
            # Convert numpy string to Python string if needed
            if isinstance(segment, np.str_):
                segment = str(segment)

            if isinstance(segment, str) and segment.strip():
                # Split into chunks
                chunks = text_splitter.split_text(segment)

                for chunk in chunks:
                    if chunk.strip():  # Only add non-empty chunks
                        documents.append({
                            'text': chunk,
                            'metadata': {
                                'pubmed_id': row['pubid'],
                                'question': row['question'],
                                'long_answer': row['long_answer'],
                                'final_decision': row['final_decision'],
                                'labels': context_data.get('labels', []),
                                'meshes': context_data.get('meshes', [])
                            }
                        })

print(f"Created {len(documents)} document chunks from PubMedQA dataset")
if documents:
    print("Sample chunk:", documents[0]['text'][:200] + "...")

# Initialize embedding model (using BAAI/bge which is state-of-the-art)
embedding_model = SentenceTransformer('BAAI/bge-base-en-v1.5')

# Generate embeddings for all chunks
texts = [doc['text'] for doc in documents]
embeddings = embedding_model.encode(texts, normalize_embeddings=True)

# Ensure embeddings is a 2D numpy array
if isinstance(embeddings, list):
    embeddings = np.array(embeddings)

# Check if embeddings is 1D (single embedding) and reshape if needed
if len(embeddings.shape) == 1:
    embeddings = embeddings.reshape(1, -1)  # Convert to 2D array with shape (1, embedding_dim)

# Create FAISS index for efficient similarity search
dimension = embeddings.shape[1]
index = faiss.IndexFlatIP(dimension)
index.add(embeddings)

print("FAISS index created with", index.ntotal, "document embeddings")

!pip install rank_bm25 cohere

from rank_bm25 import BM25Okapi
import cohere
import os
import numpy as np

# Initialize BM25 with error handling
def initialize_bm25(documents):
    """
    Safely initialize BM25 with proper document validation

    Args:
        documents: List of document dictionaries with 'text' keys

    Returns:
        BM25Okapi instance or None if initialization fails
    """
    # Filter and validate documents
    valid_docs = []
    for doc in documents:
        if isinstance(doc.get('text'), str) and doc['text'].strip():
            tokens = [token for token in doc['text'].split() if token]  # Remove empty strings
            if tokens:  # Only add if we have tokens
                valid_docs.append(tokens)

    if not valid_docs:
        print("Warning: No valid documents for BM25 - all documents were empty after processing")
        return None

    try:
        bm25 = BM25Okapi(valid_docs)
        print(f"Initialized BM25 with {len(valid_docs)} documents")
        return bm25
    except Exception as e:
        print(f"BM25 initialization failed: {str(e)}")
        return None

# Initialize BM25 (with error handling)
bm25 = initialize_bm25(documents)

# Initialize Cohere for re-ranking
try:
    co = cohere.Client(os.environ.get("COHERE_API_KEY"))
    cohere_available = True
    print("Cohere reranker initialized successfully")
except:
    print("Cohere API key not found. Will use simple hybrid search without re-ranking.")
    cohere_available = False

def hybrid_search(query, top_k=5):
    """
    Hybrid search combining vector and keyword search
    """
    # Vector search
    query_embedding = embedding_model.encode(query, normalize_embeddings=True)
    D, I = index.search(np.array([query_embedding]), top_k*3)  # Get more initially

    # BM25 search (if available)
    if bm25 is not None:
        tokenized_query = query.split()
        bm25_scores = bm25.get_scores(tokenized_query)
        bm25_top_k_indices = np.argsort(bm25_scores)[-top_k*3:][::-1]
        combined_indices = set(I[0]).union(set(bm25_top_k_indices))
    else:
        # Fallback to just vector search if BM25 failed
        combined_indices = set(I[0])

    combined_docs = [documents[idx] for idx in combined_indices]

    # Re-rank with Cohere if available
    if cohere_available and len(combined_docs) > 1:
        try:
            rerank_docs = [doc['text'] for doc in combined_docs]
            rerank_results = co.rerank(
                query=query,
                documents=rerank_docs,
                top_n=top_k,
                model="rerank-english-v2.0"
            )
            final_results = []
            for result in rerank_results:
                doc_idx = list(combined_indices)[result.index]
                final_results.append(documents[doc_idx])
            return final_results[:top_k]
        except Exception as e:
            print(f"Cohere reranking failed: {e}. Falling back to simple hybrid.")

    # Fallback ranking by combining scores (if BM25 available)
    if bm25 is not None:
        tokenized_query = query.split()
        combined_docs.sort(
            key=lambda x: (
                0.5 * D[0][list(I[0]).index(documents.index(x))] if documents.index(x) in I[0] else 0
            ) + (
                0.5 * bm25.get_scores(tokenized_query)[documents.index(x)]
            ),
            reverse=True
        )
    else:
        # Pure vector search fallback
        combined_docs.sort(
            key=lambda x: D[0][list(I[0]).index(documents.index(x))] if documents.index(x) in I[0] else 0,
            reverse=True
        )

    return combined_docs[:top_k]

# Step 4: Generation with Step-Back Prompting
from transformers import RagTokenizer, RagSequenceForGeneration
import torch

# Initialize tokenizer and model (NO retriever)
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")

from transformers import RagRetriever

class DummyRetriever(RagRetriever):
    def __init__(self):
        pass

    def retrieve(self, *args, **kwargs):
        return []

def generate_answer(question, top_k=5):
    try:
        # 1. Retrieve documents
        retrieved_docs = hybrid_search(question, top_k=top_k)

        # 2. Prepare contexts
        contexts = [str(doc['text']) for doc in retrieved_docs
                    if doc and isinstance(doc.get('text'), (str, bytes))]

        if not contexts:
            return "No relevant medical context found."

        # 3. Combine question and context
        context_text = " ".join(contexts)
        input_text = f"Question: {question}\nContext: {context_text}"

        # 4. Tokenize
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            truncation=True,
            padding="max_length",
            max_length=512
        )

        # Debug: check tokenizer output
        if inputs is None:
            print("Tokenizer returned None")
            return "Tokenizer failed to return inputs."

        if "input_ids" not in inputs or "attention_mask" not in inputs:
            print(f"Tokenizer output missing keys: {inputs.keys()}")
            return "Invalid tokenizer output."

        # 5. Generate
        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=150,
            num_beams=4
        )

        if outputs is None or len(outputs) == 0:
            return "Model failed to generate an output."

        return tokenizer.decode(outputs[0], skip_special_tokens=True)

    except Exception as e:
        print(f"Error generating answer: {str(e)}")
        return "Failed to generate an answer due to internal error."

questions = [
    "What is the mechanism of aspirin in heart attack prevention?",
    "Recommended metformin dosage for type 2 diabetes?",
    "Latest COVID-19 booster guidelines?",
    "Does aspirin helps with heartattacks?"
]

for q in questions:
    print(f"Q: {q}")
    print("RAG Answer:", generate_answer(q))
    print("-----")

# Mechanism based / factual questions
questions = [
    "How does aspirin prevent heart attacks?",
    "What is the mechanism of action of metformin?",
    "How does insulin regulate blood sugar?",
    "What is the role of ACE inhibitors in hypertension?",
    "How do statins lower cholesterol?"
]

for q in questions:
    print(f"Q: {q}")
    print("RAG Answer:", generate_answer(q))
    print("-----")

# Dosage and Treatment Guidelines
questions = [
    "What is the typical dosage of metformin for adults with type 2 diabetes?",
    "How much ibuprofen is recommended for arthritis pain?",
    "Standard treatment protocol for H. pylori infection?",
    "Recommended antibiotic for bacterial pneumonia in children?"
]

for q in questions:
    print(f"Q: {q}")
    print("RAG Answer:", generate_answer(q))
    print("-----")

# Symptoms and disease understanding
questions = [
    "What are the warning signs of a heart attack in women?",
    "Common symptoms of hypothyroidism?",
    "What causes shortness of breath in heart failure patients?"
]

for q in questions:
    print(f"Q: {q}")
    print("RAG Answer:", generate_answer(q))
    print("-----")

# Comparative clinical evidence
questions = [
    "Is metformin more effective than sulfonylureas?",
    "Does intermittent fasting improve insulin sensitivity?",
    "Which antidepressant has fewer side effects: SSRIs or SNRIs?"
]

for q in questions:
    print(f"Q: {q}")
    print("RAG Answer:", generate_answer(q))
    print("-----")

# Policy and Recommendation Based
questions = [
    "When should children receive the MMR vaccine?",
    "Does the CDC recommend RSV vaccination for adults over 60?",
]

for q in questions:
    print(f"Q: {q}")
    print("RAG Answer:", generate_answer(q))
    print("-----")

# Less Suitable of Risky Queries
questions = [
    "What are the 2025 COVID-19 guidelines?", # Real time queries
    "How would stopping metformin affect diabetic neuropathy after 3 years?", # Long reasoning chains
    "What treatment should I take if I’m 65 with hypertension and asthma?", # Highly personalized medical questions
    "What does a CT scan showing ground-glass opacity indicate?" # Image of lab value interpretations
]

for q in questions:
    print(f"Q: {q}")
    print("RAG Answer:", generate_answer(q))
    print("-----")